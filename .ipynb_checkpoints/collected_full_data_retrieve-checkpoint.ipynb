{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.curriculumDB\n",
    "db.curriculum.delete_many({})\n",
    "curriculum_collection = db.curriculumDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "#First is HBS\n",
    "\n",
    "source='https://www.hbs.edu/coursecatalog/indexcourse.html'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tdlist = soup.find_all('td')\n",
    "\n",
    "reflist = []\n",
    "for i in range(len(tdlist)):\n",
    "    if str(tdlist[i])[:27] == '<td><a href=\"/coursecatalog':\n",
    "        reflist.append(str(tdlist[i]))\n",
    "    \n",
    "#This code is necessary because HBS has course descriptions on different pages, rather than collected in one URL.\n",
    "\n",
    "linklist = []\n",
    "for i in range(len(reflist)):\n",
    "    linklist.append(reflist[i][14:37])\n",
    "    \n",
    "#Creating a list of link extensions    \n",
    "    \n",
    "descraw = []\n",
    "\n",
    "for i in range(len(linklist)):\n",
    "    link = urlopen('https://www.hbs.edu/'+ linklist[i])\n",
    "    descsoup = BeautifulSoup(link, 'html.parser')\n",
    "    descraw.append(descsoup.findAll('p'))\n",
    "    \n",
    "#navigating to link extensions and retrieving course descriptions    \n",
    "\n",
    "\n",
    "titlelist = []\n",
    "for i in range(len(reflist)):\n",
    "    titlelist.append(re.sub('<[^<]+?>', '', str(reflist[i]))) \n",
    "#titlelist\n",
    "\n",
    "\n",
    "hbsdf = pd.DataFrame(titlelist, columns=[\"Course\"])\n",
    "hbsdf['Description'] = descraw\n",
    "hbsdf['School'] = 'Harvard Business School'\n",
    "hbsdf['Source'] = source\n",
    "\n",
    "for i in range(len(hbsdf['Description'])):\n",
    "    hbsdf['Description'][i]= re.sub('<[^<]+?>', '', str(hbsdf['Description'][i]))\n",
    "    \n",
    "    hbsdf['Source'][i] = 'https://www.hbs.edu/'+ linklist[i]\n",
    "\n",
    "#This writes everything to a dataframe. Easy to export to excel. The original data is stored in a list, so it could\n",
    "#easily be written to a different kind of object, but I have used dataframes here for convenience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next is Wharton. This was the first code that I wrote, so it is a bit clunkier. Also, this includes prerequisites. (Note: this is removed in current mongoDB version)\n",
    "#I have decided to remove this from the final version, because it is outside the scope of the original project.\n",
    "\n",
    "source='https://mgmt.wharton.upenn.edu/programs/mba/course-descriptions/'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "full_text = soup.get_text()\n",
    "\n",
    "title = soup.select('h3')\n",
    "para = soup.select('p')\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))\n",
    "\n",
    "#you can see here I am scrubbing everything unnecessarily early in the process. \n",
    "\n",
    "title1df = pd.DataFrame(title, columns=['Course'])\n",
    "\n",
    "title1df['Source'] = source\n",
    "\n",
    "#The above two sections are taking the raw course catalog and putting it into a dataframe. The below sections are removing the\n",
    "#'prerequisites' columns. Because this comes from the original code, where I created an excel file with only course names, \n",
    "#there is some redundancy here. Keeping it in case the code comes in handy later. \n",
    "\n",
    "for i in range(len(title1df)):\n",
    "    if i >0:\n",
    "        title1df['Source'][i]= ''\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "prereq = []\n",
    "\n",
    "for strong_tag in soup.find_all('strong'):\n",
    "    prereq.append((strong_tag.text, strong_tag.next_sibling))\n",
    "    \n",
    "poplist = []\n",
    "prereqlist = []\n",
    "\n",
    "for i in range(len(para)):\n",
    "    if str(para[i])[:11] == '<p><strong>':\n",
    "        poplist.append(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "newpara = []\n",
    "\n",
    "for i in range(len(para)):\n",
    "    if i in poplist:\n",
    "        prereqlist.append(para[i])\n",
    "    else:\n",
    "        newpara.append(para[i])\n",
    "\n",
    "        \n",
    "#All of this is to fix the problem of classes without prerequisites. This is redundant, but the code is included here in \n",
    "#case it is useful for another school's formatting (all these websites have different formatting)\n",
    "\n",
    "#Note: In the most recent version, this doesn't actually do anything, because I didn't include a prerequisites column. Doesn't change anything important in the code. \n",
    "\n",
    "fulldf = pd.DataFrame(title[:-3], columns= ['Course'])\n",
    "\n",
    "fulldf['Source'] = source\n",
    "fulldf['School'] = 'Wharton'\n",
    "\n",
    "fulldf['Description'] = ''\n",
    "\n",
    "for i in range(len(newpara)):\n",
    "    fulldf['Description'][i]= re.sub('<[^<]+?>', '', str(newpara[i]))\n",
    "\n",
    "newpara\n",
    "fulldf['Prerequisites'] = \"\"       \n",
    "\n",
    "for i in range(len(fulldf)):\n",
    "    for j in range(len(poplist)):\n",
    "        if i == poplist[j]:\n",
    "            fulldf['Prerequisites'][i]= re.sub('<[^<]+?>', '', str(prereqlist[j]))\n",
    "        else: continue   \n",
    "fullcols = fulldf.columns.tolist()\n",
    "\n",
    "newcols = ['Course','Description',  'Source', 'School']\n",
    "\n",
    "whartondf = fulldf[newcols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stanford had problably the simplest, best-formatted website. Bless the people at Stanford who are running this website. \n",
    "\n",
    "source = 'https://exploredegrees.stanford.edu/graduateschoolofbusiness/#courseinventory'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#soup.find_all()\n",
    "title = soup.select('strong')\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))\n",
    "    \n",
    "titlelist = []\n",
    "desclist = []\n",
    "\n",
    "for i in soup.findAll('p'):\n",
    "    if str(i)[:28] == '<p class=\"courseblocktitle\">':\n",
    "        titlelist.append(i)\n",
    "    \n",
    "    elif str(i)[:27] == '<p class=\"courseblockdesc\">':\n",
    "        desclist.append(i)\n",
    "        \n",
    "        \n",
    "#Below is simply to remove html tags from the text.\n",
    "        \n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))     \n",
    "        \n",
    "stanforddf = pd.DataFrame(titlelist, columns=[\"Course\"])\n",
    "\n",
    "stanforddf['Description'] = desclist\n",
    "\n",
    "for i in range(len(stanforddf)):\n",
    "    stanforddf['Course'][i] = re.sub('<[^<]+?>', '', str( stanforddf['Course'][i]))\n",
    "    stanforddf['Description'][i] = re.sub('<[^<]+?>', '', str( stanforddf['Description'][i]))\n",
    "    \n",
    "\n",
    "stanforddf['Source'] = source\n",
    "stanforddf['School'] = 'Stanford'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "23\n",
      "57\n",
      "96\n",
      "123\n",
      "125\n",
      "127\n",
      "129\n",
      "131\n",
      "200\n",
      "208\n",
      "212\n"
     ]
    }
   ],
   "source": [
    "html = urlopen(str('https://aai.haas.berkeley.edu/scheduling/CourseSchedule.aspx?Semester=Spring&Year=2019'))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "results = soup.find_all('span')\n",
    "\n",
    "titlelist = []\n",
    "\n",
    "for result in results:\n",
    "    if len(result.text) > 4:\n",
    "    \n",
    "        titlelist.append(result.text)\n",
    "            \n",
    "linklist = []\n",
    "\n",
    "results = soup.find_all('a', class_='blue')\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        linklist.append(result['href'])\n",
    "    \n",
    "    except:\n",
    "        continue        \n",
    "                \n",
    "bigdictlist = []\n",
    "errorlist = []\n",
    "\n",
    "for i in range(len(linklist)):\n",
    "    \n",
    "    try:\n",
    "        html = urlopen(linklist[i])\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        interimlist = []\n",
    "        \n",
    "        for j in soup.find_all('p'):\n",
    "                \n",
    "            interimlist.append(j.text)\n",
    "            \n",
    "        bigdict = {linklist[i]: interimlist}\n",
    "        \n",
    "        bigdictlist.append(bigdict)\n",
    "        \n",
    "    except:\n",
    "        errorlist.append(i)\n",
    "        print(i)\n",
    "        \n",
    "testlist = []\n",
    "desclist = []\n",
    "\n",
    "for i in bigdictlist:\n",
    "    for j in i[str(tuple(i)[0])]:\n",
    "        if \"COURSE TITLE\" in j:\n",
    "            testlist.append(j)\n",
    "            desclist.append(i)\n",
    "    \n",
    "#not complete yet. However the above code does work now. The point of this is to cross reference back to\n",
    "#bigdictlist from the actual course title. \n",
    "\n",
    "coursetitlelist = []\n",
    "\n",
    "for j in range(len(bigdictlist)):\n",
    "    for i in testlist:\n",
    "        if i in bigdictlist[j][str(tuple(bigdictlist[j])[0])]:\n",
    "            \n",
    "            coursetitlelist.append(j)\n",
    "          \n",
    "\n",
    "    #The numbers currently printing are the list indices from linklist which threw a 404 error when followed. Typing linklist[number] would give the link itself             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haasdf = pd.DataFrame(testlist, columns= ['Course'])\n",
    "haasdf['School'] = 'Haas'\n",
    "haasdf['Source'] = ''\n",
    "haasdf['Description'] = desclist\n",
    "       \n",
    "sourcelist = []\n",
    "\n",
    "for i in range(len(desclist)):\n",
    "    sourcelist.append(tuple(desclist[i])[0])\n",
    "    \n",
    "haasdf['Source']=sourcelist      \n",
    "\n",
    "testlist = []\n",
    "\n",
    "for i in range(len(haasdf)):\n",
    "\n",
    "    testlist.append(haasdf['Description'][i][tuple(haasdf['Description'][i])[0]])\n",
    "\n",
    "haasdf['Description'] = testlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist = list(np.arange(1,23,1))\n",
    "\n",
    "#The simplest and best so far.\n",
    "\n",
    "source = 'https://michiganross.umich.edu/course-catalog'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "linklist = []\n",
    "\n",
    "linklist.extend(soup.find_all('a', class_=\"arrow small\"))\n",
    "\n",
    "for i in numlist:\n",
    "    source = 'https://michiganross.umich.edu/course-catalog?page=' + str(i)\n",
    "    html = urlopen(str(source))\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    linklist.extend(soup.find_all('a', class_=\"arrow small\"))\n",
    "\n",
    "desclist = []\n",
    "courselist = []\n",
    "sourcelist = []\n",
    "\n",
    "#The better written the website, the more elegant the code to scrape it. \n",
    "\n",
    "for i in linklist:\n",
    "    source = \"https://michiganross.umich.edu\" + str(i)[29:-16]\n",
    "    html = urlopen(str(source))\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    desclist.extend(soup.find_all(attrs={'name':'description'}))\n",
    "    courselist.append(str(soup.find_all(attrs={'name':'description'})).split(\" ---\",1)[0][16:] )\n",
    "    sourcelist.append(source)\n",
    "    \n",
    "rossdf = pd.DataFrame(courselist, columns = ['Course'])\n",
    "\n",
    "desc2list = []\n",
    "for i in desclist:\n",
    "    desc2list.append(str(i)[14:])\n",
    "\n",
    "rossdf['Description'] = desc2list\n",
    "rossdf['School'] = 'University of Michigan Ross'\n",
    "rossdf['Source'] = sourcelist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.concat([hbsdf, whartondf, stanforddf, haasdf, rossdf], sort=True)\n",
    "#partialdf = pd.concat([hbsdf, whartondf, stanforddf], sort=True)\n",
    "\n",
    "#partialdict = partialdf.to_dict(orient='list')\n",
    "fulldict = fulldf.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x206db9f4bc8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#db.curriculum.insert_one(partialdict)\n",
    "db.curriculum.insert_one(fulldict)\n",
    "\n",
    "\n",
    "#Note: full database should appear in mongoDB. Format for the 'Description' field is not consistent, but this is due to the underlying HTML of the pages and\n",
    "#would be more trouble than it was worth to standardize. \n",
    "\n",
    "#Especially for Haas, this field will give a dictionary with the key being the URL and the values being the content.\n",
    "#This is because each description is a different URL, but some of these URLs led to 404 errors. \n",
    "#HBS has a similar structure, but it was possible to take only content in the \"Description\" field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Note: Remaining code is unfinished and has been commented out. \n",
    "\n",
    "#source='http://www.tuck.dartmouth.edu/mba/academic-experience/elective-curriculum/elective-courses'\n",
    "\n",
    "#html = urlopen(str(source))\n",
    "#soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#divlist = []\n",
    "#for i in soup.find_all(\"div\", class_='row content' ):\n",
    " #   i.contents.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('div',class_='row content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will have to wait until I have a more finished product to test on. \n",
    "\n",
    "#import pdfkit\n",
    "\n",
    "#pdfkit.from_string(hbsdf, 'out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In class, making notes for later. Can use .strip to remove whitespace. \n",
    "\n",
    "#look for class or id when you are doing .find_all as an argument within the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
