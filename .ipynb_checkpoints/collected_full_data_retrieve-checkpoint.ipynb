{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import os\n",
    "from flask import Flask, render_template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.curriculumDB\n",
    "db.curriculum.delete_many({})\n",
    "curriculum_collection = db.curriculumDB\n",
    "\n",
    "\n",
    "if os.path.exists(\"./MBA_data.xlsx\"):\n",
    "  os.remove(\"./MBA_data.xlsx\")\n",
    "else:\n",
    "  print(\"Clean file will be produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "#First is HBS\n",
    "\n",
    "print('HBS')\n",
    "\n",
    "source='https://www.hbs.edu/coursecatalog/indexcourse.html'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tdlist = soup.find_all('td')\n",
    "\n",
    "reflist = []\n",
    "for i in range(len(tdlist)):\n",
    "    if str(tdlist[i])[:27] == '<td><a href=\"/coursecatalog':\n",
    "        reflist.append(str(tdlist[i]))\n",
    "    \n",
    "#This code is necessary because HBS has course descriptions on different pages, rather than collected in one URL.\n",
    "\n",
    "linklist = []\n",
    "for i in range(len(reflist)):\n",
    "    linklist.append(reflist[i][14:37])\n",
    "    \n",
    "#Creating a list of link extensions    \n",
    "    \n",
    "descraw = []\n",
    "\n",
    "for i in range(len(linklist)):\n",
    "    link = urlopen('https://www.hbs.edu/'+ linklist[i])\n",
    "    descsoup = BeautifulSoup(link, 'html.parser')\n",
    "    rawlist = []\n",
    "    \n",
    "    for j in descsoup.findAll('p'):\n",
    "        rawlist.append(j.text)\n",
    "        \n",
    "    descraw.append(rawlist)\n",
    "#navigating to link extensions and retrieving course descriptions    \n",
    "\n",
    "\n",
    "titlelist = []\n",
    "for i in range(len(reflist)):\n",
    "    titlelist.append(re.sub('<[^<]+?>', '', str(reflist[i]))) \n",
    "#titlelist\n",
    "\n",
    "\n",
    "hbsdf = pd.DataFrame(titlelist, columns=[\"Course\"])\n",
    "hbsdf['Description'] = descraw\n",
    "hbsdf['School'] = 'Harvard Business School'\n",
    "hbsdf['Source'] = source\n",
    "\n",
    "for i in range(len(hbsdf['Description'])):\n",
    "    hbsdf['Description'][i]= hbsdf['Description'][i]\n",
    "    \n",
    "    hbsdf['Source'][i] = 'https://www.hbs.edu/'+ linklist[i]\n",
    "\n",
    "#This writes everything to a dataframe. Easy to export to excel. The original data is stored in a list, so it could\n",
    "#easily be written to a different kind of object, but I have used dataframes here for convenience.\n",
    "\n",
    "\n",
    "for i in range(len(hbsdf['Description'])):\n",
    "    for j in range(len(hbsdf['Description'][i])):\n",
    "        hbsdf['Description'][i][j] = hbsdf['Description'][i][j].replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wharton\n"
     ]
    }
   ],
   "source": [
    "#Next is Wharton. This was the first code that I wrote, so it is a bit clunkier. Also, this includes prerequisites. (Note: this is removed in current mongoDB version)\n",
    "#I have decided to remove this from the final version, because it is outside the scope of the original project.\n",
    "\n",
    "print('Wharton')\n",
    "\n",
    "source='https://mgmt.wharton.upenn.edu/programs/mba/course-descriptions/'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "full_text = soup.get_text()\n",
    "\n",
    "title = soup.select('h3')\n",
    "para = soup.select('p')\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))\n",
    "\n",
    "#you can see here I am scrubbing everything unnecessarily early in the process. \n",
    "\n",
    "title1df = pd.DataFrame(title, columns=['Course'])\n",
    "\n",
    "title1df['Source'] = source\n",
    "\n",
    "#The above two sections are taking the raw course catalog and putting it into a dataframe. The below sections are removing the\n",
    "#'prerequisites' columns. Because this comes from the original code, where I created an excel file with only course names, \n",
    "#there is some redundancy here. Keeping it in case the code comes in handy later. \n",
    "\n",
    "for i in range(len(title1df)):\n",
    "    if i >0:\n",
    "        title1df['Source'][i]= ''\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "prereq = []\n",
    "\n",
    "for strong_tag in soup.find_all('strong'):\n",
    "    prereq.append((strong_tag.text, strong_tag.next_sibling))\n",
    "    \n",
    "poplist = []\n",
    "prereqlist = []\n",
    "\n",
    "for i in range(len(para)):\n",
    "    if str(para[i])[:11] == '<p><strong>':\n",
    "        poplist.append(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "newpara = []\n",
    "\n",
    "for i in range(len(para)):\n",
    "    if i in poplist:\n",
    "        prereqlist.append(para[i])\n",
    "    else:\n",
    "        newpara.append(para[i])\n",
    "\n",
    "        \n",
    "#All of this is to fix the problem of classes without prerequisites. This is redundant, but the code is included here in \n",
    "#case it is useful for another school's formatting (all these websites have different formatting)\n",
    "\n",
    "#Note: In the most recent version, this doesn't actually do anything, because I didn't include a prerequisites column. Doesn't change anything important in the code. \n",
    "\n",
    "fulldf = pd.DataFrame(title[:-3], columns= ['Course'])\n",
    "\n",
    "fulldf['Source'] = source\n",
    "fulldf['School'] = 'Wharton'\n",
    "\n",
    "fulldf['Description'] = ''\n",
    "\n",
    "for i in range(len(newpara)):\n",
    "    fulldf['Description'][i]= re.sub('<[^<]+?>', '', str(newpara[i]))\n",
    "\n",
    "newpara\n",
    "fulldf['Prerequisites'] = \"\"       \n",
    "\n",
    "for i in range(len(fulldf)):\n",
    "    for j in range(len(poplist)):\n",
    "        if i == poplist[j]:\n",
    "            fulldf['Prerequisites'][i]= re.sub('<[^<]+?>', '', str(prereqlist[j]))\n",
    "        else: continue   \n",
    "fullcols = fulldf.columns.tolist()\n",
    "\n",
    "newcols = ['Course','Description',  'Source', 'School']\n",
    "\n",
    "whartondf = fulldf[newcols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanford\n"
     ]
    }
   ],
   "source": [
    "#Stanford had problably the simplest, best-formatted website. Bless the people at Stanford who are running this website. \n",
    "\n",
    "print('Stanford')\n",
    "\n",
    "source = 'https://exploredegrees.stanford.edu/graduateschoolofbusiness/#courseinventory'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#soup.find_all()\n",
    "title = soup.select('strong')\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))\n",
    "    \n",
    "titlelist = []\n",
    "desclist = []\n",
    "\n",
    "for i in soup.findAll('p'):\n",
    "    if str(i)[:28] == '<p class=\"courseblocktitle\">':\n",
    "        titlelist.append(i)\n",
    "    \n",
    "    elif str(i)[:27] == '<p class=\"courseblockdesc\">':\n",
    "        desclist.append(i)\n",
    "        \n",
    "        \n",
    "#Below is simply to remove html tags from the text.\n",
    "        \n",
    "for i in range(len(titlelist)):\n",
    "    titlelist[i] = re.sub('<[^<]+?>', '', str(titlelist[i]))     \n",
    "        \n",
    "try:\n",
    "    stanforddf = pd.DataFrame(titlelist, columns=[\"Course\"])\n",
    "\n",
    "except:\n",
    "    print('Stanford webpage changed (dataframe conversion)')\n",
    "    \n",
    "stanforddf['Description'] = desclist\n",
    "\n",
    "for i in range(len(stanforddf)):\n",
    "    stanforddf['Course'][i] = re.sub('<[^<]+?>', '', str( stanforddf['Course'][i]))\n",
    "    stanforddf['Description'][i] = re.sub('<[^<]+?>', '', str( stanforddf['Description'][i]))\n",
    "    stanforddf['Description'][i] = stanforddf['Description'][i].replace('\\n', '')\n",
    "\n",
    "stanforddf['Source'] = source\n",
    "stanforddf['School'] = 'Stanford'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haas\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA201B-101A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA201B-201A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA201B-301A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA201B-401A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA204-101A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA204-201A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA204-301A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA204-401A_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA257-2_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA294-2_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA298A-1_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA299-101B_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA299-201B_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA299-301B_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/MBA299-401B_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/EWMBA295T-101_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/EWMBA298S-1_Spring19.htm Don't worry, this is expected.\n",
      "The following index threw a 404 error: i=http://courses.haas.berkeley.edu/descriptions/Descriptions/EWMBA298X-1_Spring19.htm Don't worry, this is expected.\n"
     ]
    }
   ],
   "source": [
    "print('Haas')\n",
    "\n",
    "html = urlopen(str('https://aai.haas.berkeley.edu/scheduling/CourseSchedule.aspx?Semester=Spring&Year=2019'))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "results = soup.find_all('span')\n",
    "\n",
    "titlelist = []\n",
    "\n",
    "for result in results:\n",
    "    if len(result.text) > 4:\n",
    "    \n",
    "        titlelist.append(result.text)\n",
    "            \n",
    "linklist = []\n",
    "\n",
    "results = soup.find_all('a', class_='blue')\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        linklist.append(result['href'])\n",
    "    \n",
    "    except:\n",
    "        continue        \n",
    "                \n",
    "bigdictlist = []\n",
    "errorlist = []\n",
    "\n",
    "for i in range(len(linklist)):\n",
    "    \n",
    "    try:\n",
    "        html = urlopen(linklist[i])\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        interimlist = []\n",
    "        \n",
    "        for j in soup.find_all('p'):\n",
    "                \n",
    "            interimlist.append(j.text)\n",
    "            \n",
    "        bigdict = {linklist[i]: interimlist}\n",
    "        \n",
    "        bigdictlist.append(bigdict)\n",
    "        \n",
    "    except:\n",
    "        errorlist.append(i)\n",
    "        print(\"The following index threw a 404 error: i=\" + str(linklist[i]) + \" Don't worry, this is expected.\")\n",
    "        \n",
    "testlist = []\n",
    "desclist = []\n",
    "\n",
    "for i in bigdictlist:\n",
    "    for j in i[str(tuple(i)[0])]:\n",
    "        if \"COURSE TITLE\" in j:\n",
    "            testlist.append(j)\n",
    "            desclist.append(i)\n",
    "    \n",
    "#not complete yet. However the above code does work now. The point of this is to cross reference back to\n",
    "#bigdictlist from the actual course title. \n",
    "\n",
    "coursetitlelist = []\n",
    "\n",
    "for j in range(len(bigdictlist)):\n",
    "    for i in testlist:\n",
    "        if i in bigdictlist[j][str(tuple(bigdictlist[j])[0])]:\n",
    "            \n",
    "            coursetitlelist.append(j)\n",
    "          \n",
    "    #The numbers currently printing are the list indices from linklist which threw a 404 error when followed. Typing linklist[number] would give the link itself             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haasdf = pd.DataFrame(testlist, columns= ['Course'])\n",
    "haasdf['School'] = 'Haas'\n",
    "haasdf['Source'] = ''\n",
    "haasdf['Description'] = desclist\n",
    "       \n",
    "sourcelist = []\n",
    "\n",
    "for i in range(len(desclist)):\n",
    "    sourcelist.append(tuple(desclist[i])[0])\n",
    "    \n",
    "haasdf['Source']=sourcelist      \n",
    "\n",
    "testlist = []\n",
    "\n",
    "for i in range(len(haasdf)):\n",
    "\n",
    "    testlist.append(haasdf['Description'][i][tuple(haasdf['Description'][i])[0]])\n",
    "\n",
    "haasdf['Description'] = testlist   \n",
    "\n",
    "for i in range(len(haasdf['Description'])):\n",
    "    for j in range(len(haasdf['Description'][i])):\n",
    "        haasdf['Description'][i][j] = haasdf['Description'][i][j].replace('\\n', ' ')\n",
    "        haasdf['Description'][i][j] = haasdf['Description'][i][j].replace('\\r\\n', ' ')\n",
    "        haasdf['Description'][i][j] = haasdf['Description'][i][j].replace('\\r', ' ')\n",
    "        haasdf['Description'][i][j] = haasdf['Description'][i][j].replace('\\xa0', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University of Michigan Ross\n",
      "(This one takes quite a while. Thanks for your patience)\n"
     ]
    }
   ],
   "source": [
    "numlist = list(np.arange(1,23,1))\n",
    "\n",
    "print('University of Michigan Ross')\n",
    "print('(This one takes quite a while. Thanks for your patience)')\n",
    "#The simplest and best so far.\n",
    "\n",
    "source = 'https://michiganross.umich.edu/course-catalog'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "rlinklist = []\n",
    "\n",
    "rlinklist.extend(soup.find_all('a', class_=\"arrow small\"))\n",
    "\n",
    "for i in numlist:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        source = 'https://michiganross.umich.edu/course-catalog?page=' + str(i)\n",
    "        html = urlopen(str(source))\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "        rlinklist.extend(soup.find_all('a', class_=\"arrow small\"))\n",
    "\n",
    "    except:\n",
    "        \n",
    "        print(source)\n",
    "        \n",
    "desclist = []\n",
    "courselist = []\n",
    "sourcelist = []\n",
    "\n",
    "#The better written the website, the more elegant the code to scrape it. \n",
    "\n",
    "for i in rlinklist:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        source = \"https://michiganross.umich.edu\" + str(i)[29:-16]\n",
    "        html = urlopen(str(source))\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        desclist.extend(soup.find_all(attrs={'name':'description'}))\n",
    "        courselist.append(str(soup.find_all(attrs={'name':'description'})).split(\" ---\",1)[0][16:] )\n",
    "        sourcelist.append(source)\n",
    "    \n",
    "    except:\n",
    "        print(source)\n",
    "rossdf = pd.DataFrame(courselist, columns = ['Course'])\n",
    "\n",
    "desc2list = []\n",
    "for i in desclist:\n",
    "    desc2list.append(str(i)[14:])\n",
    "\n",
    "rossdf['Description'] = desc2list\n",
    "rossdf['School'] = 'University of Michigan Ross'\n",
    "rossdf['Source'] = sourcelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIT Sloan\n"
     ]
    }
   ],
   "source": [
    "print('MIT Sloan')\n",
    "\n",
    "source = \"http://student.mit.edu/catalog/m15a.html\"\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "courselist = []\n",
    "desclist = []\n",
    "sourcelist = []\n",
    "\n",
    "for i in soup.find_all('h3'):\n",
    "   # courselist.extend([re.sub('<[^<]+?>', '', str([i]))])\n",
    "    courselist.append(i.text)\n",
    "    sourcelist.append(source)\n",
    "for i in soup.find_all('p'):    \n",
    "    desclist.append(i.text)\n",
    "\n",
    "source = \"http://student.mit.edu/catalog/m15b.html\"\n",
    "        \n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "for i in soup.find_all('h3'):\n",
    "    #courselist.extend([re.sub('<[^<]+?>', '', str([i]))])\n",
    "    courselist.append(i.text)\n",
    "    sourcelist.append(source)\n",
    "for i in soup.find_all('p'):    \n",
    "    desclist.append(i.text)    \n",
    "\n",
    "sloandf = pd.DataFrame(courselist, columns=['Course'])\n",
    "#This next part is necessary because one class, 15.329, does not have a description.\n",
    "sloandf['Description'] = ''\n",
    "\n",
    "for i in range(len(courselist)):\n",
    "    for j in range(len(desclist)):\n",
    "        if str(courselist[i])[:6] == str(desclist[j])[:6]:\n",
    "            \n",
    "            sloandf['Description'][i] = desclist[j] \n",
    "            \n",
    "sloandf['School'] = 'MIT Sloan'\n",
    "sloandf['Source'] = sourcelist\n",
    "\n",
    "\n",
    "for j in range(len(sloandf['Description'])):\n",
    "        sloandf['Description'][j] = sloandf['Description'][j].replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.concat([hbsdf, whartondf, stanforddf, haasdf, rossdf, sloandf], sort=True)\n",
    "#partialdf = pd.concat([hbsdf, whartondf, stanforddf], sort=True)\n",
    "\n",
    "fulldf.to_excel(\"./MBA_data.xlsx\")\n",
    "\n",
    "#partialdict = partialdf.to_dict(orient='list')\n",
    "fulldict = fulldf.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = fulldf.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS50 for MBAs: Computer Science for Business Leaders\n",
      "From Data to Decisions: The Role of Experiments\n",
      "Managing with Data Science\n",
      "People Analytics: Leading in a Data-Driven World\n",
      "MGTECON 536.  Data Driven Decision Making.  2 Units.\n",
      "MGTECON 634.  Machine Learning and Causal Inference.  3 Units.\n",
      "HRMGT 203.  People Analytics.  2 Units.\n",
      "MKTG 365.  Marketing Analytics.  3 Units.\n",
      "MKTG 366.  Marketing Analytics.  3 Units.\n",
      "OIT 249.  MSx: Data and Decisions.  2 Units.\n",
      "OIT 265.  Data and Decisions.  3 Units.\n",
      "OIT 267.  Data and Decisions - Accelerated.  3 Units.\n",
      "OIT 274.  Data and Decisions - Base.  4 Units.\n",
      "OIT 276.  Data and Decisions - Accelerated.  4 Units.\n",
      "OIT 367.  Business Intelligence from Big Data.  3 Units.\n",
      "OIT 521.  Data Science for Platforms.  2 Units.\n",
      "OIT 536.  Data for Action: From Insights to Applications.  2 Units.\n",
      "OIT 604.  Data, Learning, and Decision-Making.  3 Units.\n",
      "OIT 673.  Data-driven Decision Making and Applications in Healthcare.  4 Units.\n",
      "OB 310.  Org 2.0: The Analytics of Organization Design.  3 Units.\n",
      "STRAMGT 573.  Moore's Law and the Convergence of Computing and Communications; Strategic Thinking in Action.  2 Units.\n",
      "COURSE TITLE: Workplace Analytics\n",
      "COURSE TITLE: Big Data and Better Decisions\n",
      "COURSE TITLE: Marketing Analytics\n",
      "COURSE TITLE: Marketing Analytics\n",
      "COURSE TITLE: Data Science Applications in Finance\n",
      "and Accounting \n",
      "COURSE NUMBER: MBA296.8B\n",
      "\n",
      "COURSE TITLE: Data Science and Data Strategy\n",
      "COURSE TITLE: Marketing Research: Tools and Techniques for Data Collection and Analysis\n",
      "\n",
      "COURSE TITLE: Marketing Analytics\n",
      "A Brief Introduction to Computing Technologies: Crypto, Al, Quantum\n",
      "Advanced Analytics For Management Consulting\n",
      "Advanced Big Data Analytics\n",
      "Applied Business Analytics and Decisions\n",
      "Applied Business Analytics for Decision Making\n",
      "Applied Business Statistics and Analytics\n",
      "Data Mining using Regression Analysis\n",
      "Big Data in Finance\n",
      "Big Data Management: Tools and Techniques\n",
      "Big Data Manipulation and Visualization\n",
      "Business Analytics and Statistics\n",
      "Business Analytics and Statistics for Executives\n",
      "Data Analytics\n",
      "Data Mining and Applied Multivariate Analysis\n",
      "Digital Analytics\n",
      "Digital Marketing: Applications and Analytics\n",
      "Marketing Research and Analytics: Linking Data to Businss Decisions\n",
      "Marketing Engineering and Analytics\n",
      "Marketing Research and Analytics: Linking Data to Business Decisions\n",
      "Pricing Analytics and Strategy\n",
      "Data Driven Management Decision Making\n",
      "15.003 Analytics Tools\n",
      "\n",
      "15.034 Econometrics for Managers: Correlation & Causality in a Big Data World\n",
      "\n",
      "15.0341 Econometrics for Managers: Correlation and Causality in a Big Data World\n",
      "\n",
      "15.053 Optimization Methods in Business Analytics\n",
      "\n",
      "15.060 Data, Models, and Decisions\n",
      "\n",
      "15.062[J] Data Mining: Finding the Models and Predictions that Create Value\n",
      "\n",
      "15.0621 Data Mining: Finding the Models and Predictions that Create Value\n",
      "\n",
      "15.071 The Analytics Edge\n",
      "\n",
      "15.0711 The Analytics Edge\n",
      "\n",
      "15.0741 Predictive Data Analytics and Statistical Modeling\n",
      "\n",
      "15.075[J] Statistical Thinking and Data Analysis\n",
      "\n",
      "15.077[J] Statistical Learning and Data Mining\n",
      "\n",
      "15.087 Engineering Statistics and Data Science\n",
      "\n",
      "15.089 Analytics Capstone\n",
      "\n",
      "15.094[J] Robust Modeling, Optimization, and Computation\n",
      "\n",
      "15.095 Machine Learning Under a Modern Optimization Lens\n",
      "(New)\n",
      "\n",
      "15.096 Prediction: Machine Learning and Statistics\n",
      "\n",
      "15.097 Seminar in Statistics and Data Analysis\n",
      "\n",
      "15.276 Communicating with Data\n",
      "\n",
      "15.286 Communicating with Data\n",
      "\n",
      "15.312 Organizational Processes for Business Analytics\n",
      "\n",
      "15.450 Analytics of Finance\n",
      "\n",
      "15.457 Advanced Analytics of Finance\n",
      "(New)\n",
      "\n",
      "15.458 Financial Data Science and Computing I\n",
      "(New)\n",
      "\n",
      "15.459 Financial Data Science and Computing II\n",
      "(New)\n",
      "\n",
      "15.496 Practice of Finance: Data Technologies for Quantitative Finance\n",
      "\n",
      "15.570 Digital Marketing and Social Media Analytics\n",
      "\n",
      "15.572 Analytics Lab: Action Learning Seminar on Analytics, Machine Learning, and the Digital Economy\n",
      "\n",
      "15.669 Strategies for People Analytics\n",
      "\n",
      "15.680 Machine Learning: Algorithms, Applications, and Computation\n",
      "\n",
      "15.681 From Analytics to Action\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compulist = []\n",
    "datalist = []\n",
    "mllist= []\n",
    "analyticslist=[]\n",
    "\n",
    "for i in range(len(fulldf['Course'])):\n",
    "    \n",
    "    try:\n",
    "        if \"Compu\" in str(fulldf['Course'][i]):\n",
    "            compulist.append(i)\n",
    "            print(fulldf['Course'][i])\n",
    "            \n",
    "        elif \"Data\" in str(fulldf['Course'][i]):\n",
    "            datalist.append(i)\n",
    "            print(fulldf['Course'][i])\n",
    "            \n",
    "        elif \"Machine Learning\" in str(fulldf['Course'][i]):\n",
    "            mllist.append(i)\n",
    "            print(fulldf['Course'][i]) \n",
    "            \n",
    "        elif \"Analytics\" in str(fulldf['Course'][i]):\n",
    "            analyticslist.append(i)\n",
    "            print(fulldf['Course'][i])\n",
    "              \n",
    "    except:\n",
    "        print(\"Failure\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulllist = compulist\n",
    "fulllist.extend(datalist)\n",
    "fulllist.extend(mllist)\n",
    "fulllist.extend(analyticslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CliffConda\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fulldict)):\n",
    "    fulldict[i]['Data']= 0\n",
    "\n",
    "fulldf['Data'] = 0\n",
    "\n",
    "for i in fulllist:\n",
    "    fulldict[i]['Data']= 1\n",
    "    fulldf['Data'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The database can be found at \"mongodb://localhost:27017\"\n"
     ]
    }
   ],
   "source": [
    "#db.curriculum.insert_one(partialdict)\n",
    "db.curriculum.insert_many(fulldict)\n",
    "\n",
    "print('Success! The database can be found at \"mongodb://localhost:27017\"')\n",
    "\n",
    "#Note: full database should appear in mongoDB. Format for the 'Description' field is not consistent, but this is due to the underlying HTML of the pages and\n",
    "#would be more trouble than it was worth to standardize. \n",
    "\n",
    "#Especially for Haas, this field will give a dictionary with the key being the URL and the values being the content.\n",
    "#This is because each description is a different URL, but some of these URLs led to 404 errors. \n",
    "#HBS has a similar structure, but it was possible to take only content in the \"Description\" field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fulldict\n",
    "#@app.route('/')\n",
    "#def index():\n",
    "    # Store the entire team collection in a list\n",
    " #   data = db.school.find()\n",
    "  #  print(type(data))\n",
    "   # schools = list(data)\n",
    "    \n",
    "   # print(schools)\n",
    "\n",
    "    # Return the template with the teams list passed in\n",
    "    #return render_template('index.html', schools=schools)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    " #   app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Note: Remaining code is unfinished and has been commented out. \n",
    "\n",
    "#source='http://www.tuck.dartmouth.edu/mba/academic-experience/elective-curriculum/elective-courses'\n",
    "\n",
    "#html = urlopen(str(source))\n",
    "#soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#divlist = []\n",
    "#for i in soup.find_all(\"div\", class_='row content' ):\n",
    " #   i.contents.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('div',class_='row content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will have to wait until I have a more finished product to test on. \n",
    "\n",
    "#import pdfkit\n",
    "\n",
    "#pdfkit.from_string(hbsdf, 'out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In class, making notes for later. Can use .strip to remove whitespace. \n",
    "\n",
    "#look for class or id when you are doing .find_all as an argument within the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
