{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "import pymongo\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "db = client.curriculumDB\n",
    "db.curriculum.delete_many({})\n",
    "curriculum_collection = db.curriculumDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "#First is HBS\n",
    "\n",
    "source='https://www.hbs.edu/coursecatalog/indexcourse.html'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "tdlist = soup.find_all('td')\n",
    "\n",
    "reflist = []\n",
    "for i in range(len(tdlist)):\n",
    "    if str(tdlist[i])[:27] == '<td><a href=\"/coursecatalog':\n",
    "        reflist.append(str(tdlist[i]))\n",
    "    \n",
    "#This code is necessary because HBS has course descriptions on different pages, rather than collected in one URL.\n",
    "\n",
    "linklist = []\n",
    "for i in range(len(reflist)):\n",
    "    linklist.append(reflist[i][14:37])\n",
    "    \n",
    "#Creating a list of link extensions    \n",
    "    \n",
    "descraw = []\n",
    "\n",
    "for i in range(len(linklist)):\n",
    "    link = urlopen('https://www.hbs.edu/'+ linklist[i])\n",
    "    descsoup = BeautifulSoup(link, 'html.parser')\n",
    "    descraw.append(descsoup.findAll('p'))\n",
    "    \n",
    "#navigating to link extensions and retrieving course descriptions    \n",
    "\n",
    "\n",
    "titlelist = []\n",
    "for i in range(len(reflist)):\n",
    "    titlelist.append(re.sub('<[^<]+?>', '', str(reflist[i]))) \n",
    "#titlelist\n",
    "\n",
    "\n",
    "hbsdf = pd.DataFrame(titlelist, columns=[\"Course\"])\n",
    "hbsdf['Description'] = descraw\n",
    "hbsdf['School'] = 'Harvard Business School'\n",
    "hbsdf['Source'] = source\n",
    "\n",
    "for i in range(len(hbsdf['Description'])):\n",
    "    hbsdf['Description'][i]= re.sub('<[^<]+?>', '', str(hbsdf['Description'][i]))\n",
    "    \n",
    "    hbsdf['Source'][i] = 'https://www.hbs.edu/'+ linklist[i]\n",
    "\n",
    "#This writes everything to a dataframe. Easy to export to excel. The original data is stored in a list, so it could\n",
    "#easily be written to a different kind of object, but I have used dataframes here for convenience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next is Wharton. This was the first code that I wrote, so it is a bit clunkier. Also, this includes prerequisites. (Note: this is removed in current mongoDB version)\n",
    "#I have decided to remove this from the final version, because it is outside the scope of the original project.\n",
    "\n",
    "source='https://mgmt.wharton.upenn.edu/programs/mba/course-descriptions/'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "full_text = soup.get_text()\n",
    "\n",
    "title = soup.select('h3')\n",
    "para = soup.select('p')\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))\n",
    "\n",
    "#you can see here I am scrubbing everything unnecessarily early in the process. \n",
    "\n",
    "title1df = pd.DataFrame(title, columns=['Course'])\n",
    "\n",
    "title1df['Source'] = source\n",
    "\n",
    "#The above two sections are taking the raw course catalog and putting it into a dataframe. The below sections are removing the\n",
    "#'prerequisites' columns. Because this comes from the original code, where I created an excel file with only course names, \n",
    "#there is some redundancy here. Keeping it in case the code comes in handy later. \n",
    "\n",
    "for i in range(len(title1df)):\n",
    "    if i >0:\n",
    "        title1df['Source'][i]= ''\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "prereq = []\n",
    "\n",
    "for strong_tag in soup.find_all('strong'):\n",
    "    prereq.append((strong_tag.text, strong_tag.next_sibling))\n",
    "    \n",
    "poplist = []\n",
    "prereqlist = []\n",
    "\n",
    "for i in range(len(para)):\n",
    "    if str(para[i])[:11] == '<p><strong>':\n",
    "        poplist.append(i)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "newpara = []\n",
    "\n",
    "for i in range(len(para)):\n",
    "    if i in poplist:\n",
    "        prereqlist.append(para[i])\n",
    "    else:\n",
    "        newpara.append(para[i])\n",
    "\n",
    "        \n",
    "#All of this is to fix the problem of classes without prerequisites. This is redundant, but the code is included here in \n",
    "#case it is useful for another school's formatting (all these websites have different formatting)\n",
    "\n",
    "#Note: In the most recent version, this doesn't actually do anything, because I didn't include a prerequisites column. Doesn't change anything important in the code. \n",
    "\n",
    "fulldf = pd.DataFrame(title[:-3], columns= ['Course'])\n",
    "\n",
    "fulldf['Source'] = source\n",
    "fulldf['School'] = 'Wharton'\n",
    "\n",
    "fulldf['Description'] = ''\n",
    "\n",
    "for i in range(len(newpara)):\n",
    "    fulldf['Description'][i]= re.sub('<[^<]+?>', '', str(newpara[i]))\n",
    "\n",
    "newpara\n",
    "fulldf['Prerequisites'] = \"\"       \n",
    "\n",
    "for i in range(len(fulldf)):\n",
    "    for j in range(len(poplist)):\n",
    "        if i == poplist[j]:\n",
    "            fulldf['Prerequisites'][i]= re.sub('<[^<]+?>', '', str(prereqlist[j]))\n",
    "        else: continue   \n",
    "fullcols = fulldf.columns.tolist()\n",
    "\n",
    "newcols = ['Course','Description',  'Source', 'School']\n",
    "\n",
    "whartondf = fulldf[newcols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stanford had problably the simplest, best-formatted website. Bless the people at Stanford who are running this website. \n",
    "\n",
    "source = 'https://exploredegrees.stanford.edu/graduateschoolofbusiness/#courseinventory'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#soup.find_all()\n",
    "title = soup.select('strong')\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))\n",
    "    \n",
    "titlelist = []\n",
    "desclist = []\n",
    "\n",
    "for i in soup.findAll('p'):\n",
    "    if str(i)[:28] == '<p class=\"courseblocktitle\">':\n",
    "        titlelist.append(i)\n",
    "    \n",
    "    elif str(i)[:27] == '<p class=\"courseblockdesc\">':\n",
    "        desclist.append(i)\n",
    "        \n",
    "        \n",
    "#Below is simply to remove html tags from the text.\n",
    "        \n",
    "for i in range(len(title)):\n",
    "    title[i] = re.sub('<[^<]+?>', '', str(title[i]))     \n",
    "        \n",
    "stanforddf = pd.DataFrame(titlelist, columns=[\"Course\"])\n",
    "\n",
    "stanforddf['Description'] = desclist\n",
    "\n",
    "for i in range(len(stanforddf)):\n",
    "    stanforddf['Course'][i] = re.sub('<[^<]+?>', '', str( stanforddf['Course'][i]))\n",
    "    stanforddf['Description'][i] = re.sub('<[^<]+?>', '', str( stanforddf['Description'][i]))\n",
    "    \n",
    "\n",
    "stanforddf['Source'] = source\n",
    "stanforddf['School'] = 'Stanford'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-834387722652>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://aai.haas.berkeley.edu/scheduling/CourseSchedule.aspx?Semester=Spring&Year=2019'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'read'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m        \u001b[1;31m# It's a file-type object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         elif len(markup) <= 256 and (\n\u001b[0;32m    193\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34mb'<'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1007\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1009\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    869\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \"\"\"\n\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "html = urlopen(str('https://aai.haas.berkeley.edu/scheduling/CourseSchedule.aspx?Semester=Spring&Year=2019'))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "results = soup.find_all('span')\n",
    "\n",
    "titlelist = []\n",
    "\n",
    "for result in results:\n",
    "    if len(result.text) > 4:\n",
    "    \n",
    "        titlelist.append(result.text)\n",
    "            \n",
    "linklist = []\n",
    "\n",
    "results = soup.find_all('a', class_='blue')\n",
    "\n",
    "for result in results:\n",
    "    try:\n",
    "        linklist.append(result['href'])\n",
    "    \n",
    "    except:\n",
    "        continue        \n",
    "                \n",
    "bigdictlist = []\n",
    "errorlist = []\n",
    "\n",
    "for i in range(len(linklist)):\n",
    "    \n",
    "    try:\n",
    "        html = urlopen(linklist[i])\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        interimlist = []\n",
    "        \n",
    "        for j in soup.find_all('p'):\n",
    "                \n",
    "            interimlist.append(j.text)\n",
    "            \n",
    "        bigdict = {linklist[i]: interimlist}\n",
    "        \n",
    "        bigdictlist.append(bigdict)\n",
    "        \n",
    "    except:\n",
    "        errorlist.append(i)\n",
    "        print(i)\n",
    "        \n",
    "testlist = []\n",
    "desclist = []\n",
    "\n",
    "for i in bigdictlist:\n",
    "    for j in i[str(tuple(i)[0])]:\n",
    "        if \"COURSE TITLE\" in j:\n",
    "            testlist.append(j)\n",
    "            desclist.append(i)\n",
    "    \n",
    "#not complete yet. However the above code does work now. The point of this is to cross reference back to\n",
    "#bigdictlist from the actual course title. \n",
    "\n",
    "coursetitlelist = []\n",
    "\n",
    "for j in range(len(bigdictlist)):\n",
    "    for i in testlist:\n",
    "        if i in bigdictlist[j][str(tuple(bigdictlist[j])[0])]:\n",
    "            \n",
    "            coursetitlelist.append(j)\n",
    "          \n",
    "\n",
    "    #The numbers currently printing are the list indices from linklist which threw a 404 error when followed. Typing linklist[number] would give the link itself             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haasdf = pd.DataFrame(testlist, columns= ['Course'])\n",
    "haasdf['School'] = 'Haas'\n",
    "haasdf['Source'] = ''\n",
    "haasdf['Description'] = desclist\n",
    "       \n",
    "sourcelist = []\n",
    "\n",
    "for i in range(len(desclist)):\n",
    "    sourcelist.append(tuple(desclist[i])[0])\n",
    "    \n",
    "haasdf['Source']=sourcelist      \n",
    "\n",
    "testlist = []\n",
    "\n",
    "for i in range(len(haasdf)):\n",
    "\n",
    "    testlist.append(haasdf['Description'][i][tuple(haasdf['Description'][i])[0]])\n",
    "\n",
    "haasdf['Description'] = testlist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist = list(np.arange(1,23,1))\n",
    "\n",
    "#The simplest and best so far.\n",
    "\n",
    "source = 'https://michiganross.umich.edu/course-catalog'\n",
    "\n",
    "html = urlopen(str(source))\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "linklist = []\n",
    "\n",
    "linklist.extend(soup.find_all('a', class_=\"arrow small\"))\n",
    "\n",
    "for i in numlist:\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        source = 'https://michiganross.umich.edu/course-catalog?page=' + str(i)\n",
    "        html = urlopen(str(source))\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "        linklist.extend(soup.find_all('a', class_=\"arrow small\"))\n",
    "\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(source)\n",
    "        \n",
    "desclist = []\n",
    "courselist = []\n",
    "sourcelist = []\n",
    "\n",
    "#The better written the website, the more elegant the code to scrape it. \n",
    "\n",
    "for i in linklist:\n",
    "    source = \"https://michiganross.umich.edu\" + str(i)[29:-16]\n",
    "    html = urlopen(str(source))\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    desclist.extend(soup.find_all(attrs={'name':'description'}))\n",
    "    courselist.append(str(soup.find_all(attrs={'name':'description'})).split(\" ---\",1)[0][16:] )\n",
    "    sourcelist.append(source)\n",
    "    \n",
    "rossdf = pd.DataFrame(courselist, columns = ['Course'])\n",
    "\n",
    "desc2list = []\n",
    "for i in desclist:\n",
    "    desc2list.append(str(i)[14:])\n",
    "\n",
    "rossdf['Description'] = desc2list\n",
    "rossdf['School'] = 'University of Michigan Ross'\n",
    "rossdf['Source'] = sourcelist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.concat([hbsdf, whartondf, stanforddf, haasdf, rossdf], sort=True)\n",
    "#partialdf = pd.concat([hbsdf, whartondf, stanforddf], sort=True)\n",
    "\n",
    "#partialdict = partialdf.to_dict(orient='list')\n",
    "fulldict = fulldf.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db.curriculum.insert_one(partialdict)\n",
    "db.curriculum.insert_one(fulldict)\n",
    "\n",
    "\n",
    "#Note: full database should appear in mongoDB. Format for the 'Description' field is not consistent, but this is due to the underlying HTML of the pages and\n",
    "#would be more trouble than it was worth to standardize. \n",
    "\n",
    "#Especially for Haas, this field will give a dictionary with the key being the URL and the values being the content.\n",
    "#This is because each description is a different URL, but some of these URLs led to 404 errors. \n",
    "#HBS has a similar structure, but it was possible to take only content in the \"Description\" field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Note: Remaining code is unfinished and has been commented out. \n",
    "\n",
    "#source='http://www.tuck.dartmouth.edu/mba/academic-experience/elective-curriculum/elective-courses'\n",
    "\n",
    "#html = urlopen(str(source))\n",
    "#soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#divlist = []\n",
    "#for i in soup.find_all(\"div\", class_='row content' ):\n",
    " #   i.contents.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('div',class_='row content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will have to wait until I have a more finished product to test on. \n",
    "\n",
    "#import pdfkit\n",
    "\n",
    "#pdfkit.from_string(hbsdf, 'out.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In class, making notes for later. Can use .strip to remove whitespace. \n",
    "\n",
    "#look for class or id when you are doing .find_all as an argument within the function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
